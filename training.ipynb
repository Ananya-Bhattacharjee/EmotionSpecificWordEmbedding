{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3wXAR4tnMvl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import zipfile \n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBsBYaaezTEL",
    "outputId": "36b9eacd-2687-4b3d-891e-caa5c6017304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ug1pEEjXW5pE",
    "outputId": "d1bba266-a1d4-4b32-f6cd-7cd918f512c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if \"GPU\" not in device_name:\n",
    "    print(\"GPU device not found\")\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flD5o-YW8cSq"
   },
   "source": [
    "# Load the data\n",
    "we have 3 training datasets and need to combined them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNl8hLHkwNQY"
   },
   "outputs": [],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPu0uzXGp4L5"
   },
   "outputs": [],
   "source": [
    "# load dataset 1\n",
    "with zipfile.ZipFile('/content/drive/MyDrive/CSC2515/part2/2019_Train_EmotionPush.zip','r') as z:\n",
    "    with z.open('EmotionPush/emotionpush.json') as f:\n",
    "        data = json.loads(f.read())\n",
    "        data = flatten(data)\n",
    "        df1 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXHKUZdRsv0c"
   },
   "outputs": [],
   "source": [
    "# load dataset 2\n",
    "with zipfile.ZipFile('/content/drive/MyDrive/CSC2515/part2/2019_Train_Friends.zip','r') as z:\n",
    "    with z.open('Friends/friends.json') as f:\n",
    "        data = json.loads(f.read())\n",
    "        data = flatten(data)\n",
    "        df2 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nA1Y3kZwt-NX"
   },
   "outputs": [],
   "source": [
    "# load dataset 3\n",
    "df3 = pd.read_csv('/content/drive/MyDrive/CSC2515/part2/text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fp0SUOMJuHkO"
   },
   "outputs": [],
   "source": [
    "# combine all three datasets\n",
    "df1 = df1.drop(['speaker', 'annotation'], axis=1)\n",
    "df2 = df2.drop(['speaker', 'annotation'], axis=1)\n",
    "df1 = pd.concat([df1, df2], axis=0)\n",
    "df3 = df3.drop(['tweet_id', 'author'], axis=1)\n",
    "df3.columns = ['emotion', 'utterance']\n",
    "df = pd.concat([df1, df3], axis=0)\n",
    "# df.to_csv('/content/drive/MyDrive/CSC2515/part2/emotion_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GdzDR8u8-nZ"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/content/drive/MyDrive/CSC2515/part2/emotion_combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdTAq11C82nm"
   },
   "source": [
    "# Data Proprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPd1KOA1xs5x"
   },
   "outputs": [],
   "source": [
    "# remove @users \n",
    "df['utterance'] = df.utterance.str.replace('@[a-zA-Z]+', 'someone')\n",
    "\n",
    "# drop missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_R9RagXe_-pT"
   },
   "outputs": [],
   "source": [
    "# count the words in each sample\n",
    "df['words'] = df['utterance'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOITxs7_khIk"
   },
   "outputs": [],
   "source": [
    "# data cleaning functions\n",
    "def clean_numbers(x):\n",
    "    '''\n",
    "    replace number with # sign\n",
    "    '''\n",
    "    x = re.sub('[0-9]{3,}', '### ', x)\n",
    "    x = re.sub('[0-9]{2}', ' ## ', x)\n",
    "    return x\n",
    "\n",
    "def clean_punc(x):\n",
    "    '''\n",
    "    replace punctuation with space\n",
    "    '''\n",
    "    x = str(x)\n",
    "    for punct in string.punctuation:\n",
    "        x = x.replace(punct, ' ')\n",
    "    return x\n",
    "\n",
    "def preprocess(df, feature):\n",
    "    '''\n",
    "    preprocess the feature column in df\n",
    "    remove punctuations, change to lowercase, repalce numbers with #\n",
    "    fill with 'None' if the string is empty\n",
    "    '''\n",
    "    df[feature] = df[feature].map(clean_punc)\n",
    "    df[feature] = df[feature].str.lower()\n",
    "    df[feature] = df[feature].map(clean_numbers)\n",
    "    df[feature] = df[feature].map(lambda x: 'None' if x == '' else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zmBINbCnQzU"
   },
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "df = preprocess(df, 'utterance')\n",
    "\n",
    "# shuffle the data\n",
    "df = df.sample(frac=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTEftjwR9Ouk"
   },
   "outputs": [],
   "source": [
    "# get unique number of classes (17)\n",
    "num_cls = len(df.emotion.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBBo4oOJlI-9"
   },
   "outputs": [],
   "source": [
    "# encode the class labels\n",
    "LE = LabelEncoder()\n",
    "df['emotion_code'] = LE.fit_transform(df.emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dm-hb-U4ffX_"
   },
   "outputs": [],
   "source": [
    "# train validation split\n",
    "mask = np.random.rand(df.shape[0]) < 0.9\n",
    "df_train = df[mask]\n",
    "df_val = df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7cm30XoyG4o"
   },
   "outputs": [],
   "source": [
    "# load the data into tensorflow dataset \n",
    "train = tf.data.Dataset.from_tensor_slices((df_train.utterance.values, df_train.emotion_code.values))\n",
    "val = tf.data.Dataset.from_tensor_slices((df_val.utterance.values, df_val.emotion_code.values))\n",
    "\n",
    "# convert class labels into one-hot encoding\n",
    "train = train.map(lambda x, y: (x, tf.one_hot(y, depth=num_cls)))\n",
    "val = val.map(lambda x, y: (x, tf.one_hot(y, depth=num_cls)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_Na6YQPnn6s",
    "outputId": "ca687887-6290-406c-843c-ccd6b05a4866"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9210"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many words appeared at least 3 times\n",
    "vec = CountVectorizer(ngram_range=(1, 1), min_df=3).fit(df['utterance'].values)\n",
    "len(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLxMPWSQ39lv"
   },
   "outputs": [],
   "source": [
    "# keras text vectorization layer\n",
    "# max token is set to contain only words with frequency large than 3\n",
    "max_token = len(vec.get_feature_names()) + 2\n",
    "vectorize = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_token, standardize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fwF_QV74dAA"
   },
   "outputs": [],
   "source": [
    "# adapt the text vectorization layer on the training data\n",
    "vectorize.adapt(df['utterance'].values)\n",
    "words = vectorize.get_vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlhBmEPqYy7Y"
   },
   "outputs": [],
   "source": [
    "# cach the dataset into memory\n",
    "cached_train = train.batch(1024).cache()\n",
    "cached_val = val.batch(1024).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nF_D6n2Hn3P"
   },
   "source": [
    "# word embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wTfpvNzdLME"
   },
   "outputs": [],
   "source": [
    "# generate embedding model\n",
    "# this would be the model we are interested in\n",
    "def embedding_model(regularizers):\n",
    "    '''\n",
    "    generate a word embedding layer that takes input of string and outputs word embedding\n",
    "    '''\n",
    "    input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "    x = vectorize(input)\n",
    "    output = tf.keras.layers.Embedding(len(words) + 2, \n",
    "                                       64, \n",
    "                                       embeddings_regularizer=regularizers,\n",
    "                                       )(x)\n",
    "    return tf.keras.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6PI5pT1nmeW"
   },
   "outputs": [],
   "source": [
    "def get_embedding_dict(model):\n",
    "    '''\n",
    "    extract the word embedding dictionary from the model\n",
    "    '''\n",
    "    embedding_dict = {}\n",
    "    for word in words[2:]:\n",
    "        embedding_dict[word] = model.layers[0](tf.constant(word)).numpy().flatten()\n",
    "    return embedding_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kv3dTLY3HSYq"
   },
   "source": [
    "# Average model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPVuqxmxRVQ_"
   },
   "outputs": [],
   "source": [
    "def Average_model():\n",
    "    '''\n",
    "    return a training model that uses average layer to summarize the sentence into one vector\n",
    "    uses categorical cross entropy as loss function\n",
    "    uses macro F1 score as metrics\n",
    "    '''\n",
    "    model = tf.keras.Sequential([\n",
    "        embedding_model(\n",
    "            None,\n",
    "            # tf.keras.regularizers.L2(1e-7)\n",
    "        ),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_cls, \n",
    "                              activation='softmax', \n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.01))                           \n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.3),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tfa.metrics.F1Score(num_cls, average='macro')]\n",
    "              )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rpuLkQkd8k3"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score', patience=10, mode='max')\n",
    "\n",
    "ave_model = Average_model()\n",
    "\n",
    "ave_model.fit(cached_train, epochs=1000, verbose=2, validation_data=cached_val, \n",
    "              callbacks=[early_stopping],\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abNmoto0ozj_"
   },
   "outputs": [],
   "source": [
    "ave_dict = get_embedding_dict(ave_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjRbGXhppoA3"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/CSC2515/part2/ave_dict', 'wb') as fp:\n",
    "    pickle.dump(ave_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__vfuc1RHLDz"
   },
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXhkftrO1nrJ"
   },
   "outputs": [],
   "source": [
    "def LSTM_model():\n",
    "    '''\n",
    "    return a training model that uses LSTM layer to summarize the sentence into one vector\n",
    "    uses categorical cross entropy as loss function\n",
    "    uses macro F1 score as metrics\n",
    "    '''\n",
    "    model = tf.keras.Sequential([\n",
    "        embedding_model(\n",
    "            None,\n",
    "            # tf.keras.regularizers.l2(1e-7),\n",
    "            ),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        # tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(num_cls, activation='softmax')                           \n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.3),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tfa.metrics.F1Score(num_cls, average='macro')]\n",
    "              )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ynbk7OO_DZr3"
   },
   "outputs": [],
   "source": [
    "lstm_model = LSTM_model()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score', patience=100, mode='max')\n",
    "\n",
    "lstm_model.fit(cached_train, epochs=1000, validation_data=cached_val, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nwxu9g1LFfZM"
   },
   "outputs": [],
   "source": [
    "lstm_dict = get_embedding_dict(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEg6zGg4Fjh4"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/CSC2515/part2/lstm_dict', 'wb') as fp:\n",
    "    pickle.dump(lstm_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUObmqPMG_tm"
   },
   "source": [
    "# attention functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j50ZYHMLxAwT"
   },
   "outputs": [],
   "source": [
    "# attention model functions\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    '''\n",
    "    generate angles for positional encoding\n",
    "    '''\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    '''\n",
    "    generate positional encoding\n",
    "    '''\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def add_position():\n",
    "    '''\n",
    "    return a layer that adds positional encoding into embeddings\n",
    "    the layer takes sequence of word embeddings as input, \n",
    "    outputs positional encoded embeddings\n",
    "    '''\n",
    "    x = tf.keras.layers.Input(shape=(None, 64))\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    x_positioned = x*8 + positional_encoding(20000, 64)[:, :seq_len, :]\n",
    "    return tf.keras.Model(inputs=x, outputs=x_positioned)\n",
    "\n",
    "def attention_layer():\n",
    "    '''\n",
    "    return a layer that apply attention algorithm\n",
    "    the layer takes sequence of q, k and v as input\n",
    "    outputs sequence of attentions\n",
    "    '''\n",
    "    q = tf.keras.layers.Input(shape=(None, None, None))\n",
    "    k = tf.keras.layers.Input(shape=(None, None, None))\n",
    "    v = tf.keras.layers.Input(shape=(None, None, None))\n",
    "    \n",
    "    matmul = tf.matmul(q, k, transpose_b=True)\n",
    "    scaled_attention_logits = matmul / 8\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return tf.keras.Model(inputs=[q, k, v],  outputs=output)\n",
    "\n",
    "def multi_head(d_model, num_heads):\n",
    "    '''\n",
    "    The outer wrapper of attention model\n",
    "    takes sequence of word embeddings as input\n",
    "    convert embeddings into q, k and v\n",
    "    split them into multiple parts (number of heads)\n",
    "    apply attention algorithm\n",
    "    outputs sequence of attentions\n",
    "    '''\n",
    "    x = tf.keras.layers.Input(shape=(None, 64))\n",
    "    batch_size, seq_len = tf.shape(x)[:2]\n",
    "    depth = d_model // num_heads\n",
    "\n",
    "    # q.shape = (batch_size, seq_len, d_model); same as k and v\n",
    "    q = tf.keras.layers.Dense(64)(x)  \n",
    "    k = tf.keras.layers.Dense(64)(x)  \n",
    "    v = tf.keras.layers.Dense(64)(x)  \n",
    "\n",
    "    # convert to (batch_size, num_heads, seq_len, depth)\n",
    "    q = tf.transpose(tf.reshape(q, (batch_size, seq_len, num_heads, depth)), perm=[0, 2, 1, 3])  \n",
    "    k = tf.transpose(tf.reshape(k, (batch_size, seq_len, num_heads, depth)), perm=[0, 2, 1, 3])  \n",
    "    v = tf.transpose(tf.reshape(v, (batch_size, seq_len, num_heads, depth)), perm=[0, 2, 1, 3])  \n",
    "\n",
    "    # scaled_attention.shape = (batch_size, num_heads, seq_len, depth)\n",
    "    scaled_attention = attention_layer()([q, k, v])\n",
    "\n",
    "    # convert to (batch_size, seq_len, num_heads, depth)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "    # convert back to (batch_size, seq_len, d_model)\n",
    "    concat_attention = tf.reshape(scaled_attention, (batch_size, seq_len, d_model))  \n",
    "\n",
    "    output = tf.keras.layers.Dense(64)(concat_attention)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    return tf.keras.Model(inputs=x, outputs=output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9NWzb-KG7cC"
   },
   "source": [
    "# Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4itM-UtxdyO"
   },
   "outputs": [],
   "source": [
    "def attention_model():\n",
    "    '''\n",
    "    return a training model that uses attention layer to summarize the sentence into one vector\n",
    "    uses categorical cross entropy as loss function\n",
    "    uses macro F1 score as metrics\n",
    "    '''\n",
    "    model = tf.keras.Sequential([\n",
    "        embedding_model(\n",
    "            None,\n",
    "            # tf.keras.regularizers.L2(1e-7),\n",
    "            ),\n",
    "        add_position(),\n",
    "        multi_head(64, 1),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(num_cls, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))                           \n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tfa.metrics.F1Score(num_cls, average='macro')]\n",
    "              )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHpvjKyhyFb3"
   },
   "outputs": [],
   "source": [
    "att_model = attention_model()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score', patience=10, mode='max')\n",
    "\n",
    "att_model.fit(cached_train, epochs=1000, validation_data=cached_val, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3ijeohlHk3x"
   },
   "outputs": [],
   "source": [
    "att_dict = get_embedding_dict(att_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZPqL0l4Hnm4"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/CSC2515/part2/att_dict', 'wb') as fp:\n",
    "    pickle.dump(att_dict, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZgYCd65G3Ju"
   },
   "source": [
    "# Multi heads attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aHa4ASnywzj"
   },
   "outputs": [],
   "source": [
    "def multi_attention_model():\n",
    "    '''\n",
    "    return a training model that uses multi heads attention layer to summarize the sentence into one vector\n",
    "    uses categorical cross entropy as loss function\n",
    "    uses macro F1 score as metrics\n",
    "    '''\n",
    "    model = tf.keras.Sequential([\n",
    "        embedding_model(\n",
    "            None,\n",
    "            # tf.keras.regularizers.L2(1e-7),\n",
    "            ),\n",
    "        add_position(),\n",
    "        multi_head(64, 8),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(num_cls, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))                           \n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tfa.metrics.F1Score(num_cls, average='macro')]\n",
    "              )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7SHTdBqHzck"
   },
   "outputs": [],
   "source": [
    "mtatt_model = multi_attention_model()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_score', patience=10, mode='max')\n",
    "mtatt_model.fit(cached_train, epochs=1000, validation_data=cached_val, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0R2foCkH-YD"
   },
   "outputs": [],
   "source": [
    "mtatt_dict = get_embedding_dict(mtatt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_VydHe6I5cY"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/CSC2515/part2/mtatt_dict', 'wb') as fp:\n",
    "    pickle.dump(mtatt_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWzpyJQ7JBSI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "csc2515part2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
